{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6518808,"sourceType":"datasetVersion","datasetId":3768491}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing Libraries\nfrom tensorflow import keras\nfrom tensorflow.keras.applications.densenet import DenseNet121\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Flatten, MaxPooling2D, Dense, Dropout, GlobalAveragePooling2D\nfrom tensorflow.keras import optimizers, losses\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.preprocessing import image\n\nimport pickle\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-06T18:31:58.469527Z","iopub.execute_input":"2024-05-06T18:31:58.469951Z","iopub.status.idle":"2024-05-06T18:31:58.478798Z","shell.execute_reply.started":"2024-05-06T18:31:58.469921Z","shell.execute_reply":"2024-05-06T18:31:58.477388Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# Base Path for all files\ndata_dir = '/kaggle/input/lumpy-skin-images-dataset/Lumpy Skin Images Dataset'","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:31:58.480777Z","iopub.execute_input":"2024-05-06T18:31:58.481116Z","iopub.status.idle":"2024-05-06T18:31:58.492895Z","shell.execute_reply.started":"2024-05-06T18:31:58.481089Z","shell.execute_reply":"2024-05-06T18:31:58.491130Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"###### Using ImageDataGenerator to load the Images for Training and Testing the CNN Model\ndatagenerator = {\n    \"train\": ImageDataGenerator(horizontal_flip=True,\n                                vertical_flip=True,\n                                rescale=1. / 255,\n                                validation_split=0.1,\n                                shear_range=0.1,\n                                zoom_range=0.1,\n                                width_shift_range=0.1,\n                                height_shift_range=0.1,\n                                rotation_range=30,\n                               ).flow_from_directory(directory=data_dir,\n                                                     target_size=(256, 256),\n                                                     subset='training',\n                                                    ),\n\n    \"valid\": ImageDataGenerator(rescale=1 / 255,\n                                validation_split=0.1,\n                               ).flow_from_directory(directory=data_dir,\n                                                     target_size=(256, 256),\n                                                     subset='validation',\n                                                    ),\n}","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:31:58.495947Z","iopub.execute_input":"2024-05-06T18:31:58.497239Z","iopub.status.idle":"2024-05-06T18:31:58.563958Z","shell.execute_reply.started":"2024-05-06T18:31:58.497198Z","shell.execute_reply":"2024-05-06T18:31:58.562838Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Found 922 images belonging to 2 classes.\nFound 102 images belonging to 2 classes.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"1. Data Preprocessing:\n\n**Image Data Generator:**\n\nDefine an ImageDataGenerator object to augment your training data. This helps the model generalize better and avoid overfitting. Use techniques like random flips, rotations, and scaling to create variations of your images.","metadata":{}},{"cell_type":"code","source":"train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\nvalidation_datagen = ImageDataGenerator(rescale=1./255,\n                                         validation_split=0.1) \n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:31:58.565540Z","iopub.execute_input":"2024-05-06T18:31:58.565962Z","iopub.status.idle":"2024-05-06T18:31:58.573762Z","shell.execute_reply.started":"2024-05-06T18:31:58.565921Z","shell.execute_reply":"2024-05-06T18:31:58.572176Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"**Data Flow:**\n\nUse the generators with flow_from_directory to create data generators that will automatically load and preprocess your images based on the class labels in the folders.","metadata":{}},{"cell_type":"code","source":"train_generator = train_datagen.flow_from_directory(\n    data_dir,  # Base directory\n    target_size=(224, 224),  # Resize images to 224x224 (common for DenseNet)\n    batch_size=32,  # Train in batches of 32\n    class_mode='binary'  # Binary classification (Lumpy Skin or Normal)\n)\n\nvalidation_generator = validation_datagen.flow_from_directory(\n    data_dir,\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='binary',\n    subset='validation'  # Use validation split from directory\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:31:58.577533Z","iopub.execute_input":"2024-05-06T18:31:58.577963Z","iopub.status.idle":"2024-05-06T18:31:58.638177Z","shell.execute_reply.started":"2024-05-06T18:31:58.577931Z","shell.execute_reply":"2024-05-06T18:31:58.637033Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Found 1024 images belonging to 2 classes.\nFound 102 images belonging to 2 classes.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"2. Model Building:\n\n**Base Model (Transfer Learning):**\nDefine your base model using DenseNet121. Since you already have pre-trained weights for DenseNet, this is a good choice. Set include_top=False to exclude the final classification layers.","metadata":{}},{"cell_type":"code","source":"base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:31:58.639803Z","iopub.execute_input":"2024-05-06T18:31:58.640149Z","iopub.status.idle":"2024-05-06T18:32:00.834668Z","shell.execute_reply.started":"2024-05-06T18:31:58.640121Z","shell.execute_reply":"2024-05-06T18:32:00.833341Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"**Freezing Base Model Layers (Optional):**\n\nConsider freezing the base model layers to prevent them from being updated during training. This can be helpful if you have a limited dataset to avoid overfitting the pre-trained weights.","metadata":{}},{"cell_type":"code","source":"base_model.trainable = False  # Freeze base model layers\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:32:00.837354Z","iopub.execute_input":"2024-05-06T18:32:00.837846Z","iopub.status.idle":"2024-05-06T18:32:00.856910Z","shell.execute_reply.started":"2024-05-06T18:32:00.837806Z","shell.execute_reply":"2024-05-06T18:32:00.855385Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"**Adding Classification Layers:**\n1. Create a Sequential model and add the pre-trained base model (without the top layers).\n1. Add additional convolutional layers for further feature extraction if needed.\n1. Use GlobalAveragePooling2D for efficient feature extraction.\n1. Add Dense layers with dropout for regularization. Finally, add a Dense layer with 1 unit and sigmoid activation for binary classification (Lumpy Skin or Normal).","metadata":{}},{"cell_type":"code","source":"model = Sequential()\nmodel.add(base_model)\nmodel.add(GlobalAveragePooling2D())\nmodel.add(Dense(128, activation='relu'))  # Additional dense layer with ReLU activation\nmodel.add(Dropout(0.5))  # Dropout for regularization\nmodel.add(Dense(1, activation='sigmoid'))  # Output layer with sigmoid activation\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:32:00.858246Z","iopub.execute_input":"2024-05-06T18:32:00.858670Z","iopub.status.idle":"2024-05-06T18:32:00.883539Z","shell.execute_reply.started":"2024-05-06T18:32:00.858640Z","shell.execute_reply":"2024-05-06T18:32:00.882157Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"3. Training the Model:\n\n**Model Training:**\n\nUse the model.fit function to train the model with your training data generator (train_generator) and validation data generator (validation_generator). Specify the number of epochs (iterations over the data) and steps per epoch","metadata":{}},{"cell_type":"code","source":"model.fit(train_generator,\n          epochs=10,  # Adjust based on dataset size and validation performance\n          validation_data=validation_generator)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:32:00.885598Z","iopub.execute_input":"2024-05-06T18:32:00.886032Z","iopub.status.idle":"2024-05-06T18:54:16.637806Z","shell.execute_reply.started":"2024-05-06T18:32:00.886003Z","shell.execute_reply":"2024-05-06T18:54:16.636420Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 4s/step - accuracy: 0.6599 - loss: 0.7407 - val_accuracy: 0.8922 - val_loss: 0.2925\nEpoch 2/10\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 4s/step - accuracy: 0.8470 - loss: 0.3693 - val_accuracy: 0.8922 - val_loss: 0.2366\nEpoch 3/10\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 4s/step - accuracy: 0.9001 - loss: 0.2667 - val_accuracy: 0.9216 - val_loss: 0.1941\nEpoch 4/10\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 4s/step - accuracy: 0.8681 - loss: 0.3130 - val_accuracy: 0.9118 - val_loss: 0.2002\nEpoch 5/10\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 3s/step - accuracy: 0.8952 - loss: 0.2548 - val_accuracy: 0.9510 - val_loss: 0.1566\nEpoch 6/10\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 3s/step - accuracy: 0.9015 - loss: 0.2581 - val_accuracy: 0.9216 - val_loss: 0.1872\nEpoch 7/10\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 3s/step - accuracy: 0.8982 - loss: 0.2567 - val_accuracy: 0.9314 - val_loss: 0.1583\nEpoch 8/10\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 3s/step - accuracy: 0.9046 - loss: 0.2401 - val_accuracy: 0.8922 - val_loss: 0.2372\nEpoch 9/10\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 3s/step - accuracy: 0.9110 - loss: 0.2330 - val_accuracy: 0.9706 - val_loss: 0.1255\nEpoch 10/10\n\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 3s/step - accuracy: 0.9198 - loss: 0.2065 - val_accuracy: 0.9608 - val_loss: 0.1451\n","output_type":"stream"},{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7d7ad9f5b880>"},"metadata":{}}]},{"cell_type":"markdown","source":"**Empty Data Generators:**\n\n","metadata":{}},{"cell_type":"code","source":"for sample in train_generator:\n    images, labels = sample\n    # Print the shape of images and labels to confirm data\n    print(images.shape, labels.shape)\n    break  # Only print a few samples\n\nfor sample in validation_generator:\n    images, labels = sample\n    # Print the shape of images and labels to confirm data\n    print(images.shape, labels.shape)\n    break  # Only print a few samples\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:54:16.639709Z","iopub.execute_input":"2024-05-06T18:54:16.640069Z","iopub.status.idle":"2024-05-06T18:54:17.224264Z","shell.execute_reply.started":"2024-05-06T18:54:16.640040Z","shell.execute_reply":"2024-05-06T18:54:17.222829Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"(32, 224, 224, 3) (32,)\n(32, 224, 224, 3) (32,)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nprint(os.listdir(data_dir))  # List contents of your data directory\n","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:54:17.228119Z","iopub.execute_input":"2024-05-06T18:54:17.228650Z","iopub.status.idle":"2024-05-06T18:54:17.236022Z","shell.execute_reply.started":"2024-05-06T18:54:17.228606Z","shell.execute_reply":"2024-05-06T18:54:17.234168Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"['Normal Skin', 'Lumpy Skin']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Viewing the summary of the model\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:55:08.895593Z","iopub.execute_input":"2024-05-06T18:55:08.896025Z","iopub.status.idle":"2024-05-06T18:55:08.951316Z","shell.execute_reply.started":"2024-05-06T18:55:08.895997Z","shell.execute_reply":"2024-05-06T18:55:08.950203Z"},"trusted":true},"execution_count":61,"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_2\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ densenet121 (\u001b[38;5;33mFunctional\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │     \u001b[38;5;34m7,037,504\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d_2      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m131,200\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ densenet121 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,037,504</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ global_average_pooling2d_2      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,200</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n├─────────────────────────────────┼────────────────────────┼───────────────┤\n│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n└─────────────────────────────────┴────────────────────────┴───────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,431,493\u001b[0m (28.35 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,431,493</span> (28.35 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m131,329\u001b[0m (513.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">131,329</span> (513.00 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m7,037,504\u001b[0m (26.85 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,037,504</span> (26.85 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m262,660\u001b[0m (1.00 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">262,660</span> (1.00 MB)\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"4. Evaluation and Saving\n**Evaluate the Model:**\n\nUse the **model.evaluate** function to evaluate the model's performance on the validation data.","metadata":{}},{"cell_type":"code","source":"model.evaluate","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:54:17.237965Z","iopub.execute_input":"2024-05-06T18:54:17.238479Z","iopub.status.idle":"2024-05-06T18:54:17.247857Z","shell.execute_reply.started":"2024-05-06T18:54:17.238397Z","shell.execute_reply":"2024-05-06T18:54:17.246316Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"<bound method TensorFlowTrainer.evaluate of <Sequential name=sequential_2, built=True>>"},"metadata":{}}]},{"cell_type":"markdown","source":"**Save the Model:**\n\nUse model.save('lumpy_skin_classifier.h5') to save the trained model for future use","metadata":{}},{"cell_type":"code","source":"model.save('/kaggle/working/lumpy_skin_classifier123.h5')","metadata":{"execution":{"iopub.status.busy":"2024-05-06T18:54:38.336055Z","iopub.execute_input":"2024-05-06T18:54:38.336488Z","iopub.status.idle":"2024-05-06T18:54:38.775320Z","shell.execute_reply.started":"2024-05-06T18:54:38.336457Z","shell.execute_reply":"2024-05-06T18:54:38.773484Z"},"trusted":true},"execution_count":60,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/lumpy_skin_classifier123.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/h5py/_hl/group.py:183\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    180\u001b[0m         parent_path, name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    181\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[0;32m--> 183\u001b[0m dsid \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_new_dset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/h5py/_hl/dataset.py:163\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[0;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     sid \u001b[38;5;241m=\u001b[39m h5s\u001b[38;5;241m.\u001b[39mcreate_simple(shape, maxshape)\n\u001b[0;32m--> 163\u001b[0m dset_id \u001b[38;5;241m=\u001b[39m \u001b[43mh5d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdcpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, Empty)):\n\u001b[1;32m    166\u001b[0m     dset_id\u001b[38;5;241m.\u001b[39mwrite(h5s\u001b[38;5;241m.\u001b[39mALL, h5s\u001b[38;5;241m.\u001b[39mALL, data)\n","File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n","File \u001b[0;32mh5py/h5d.pyx:137\u001b[0m, in \u001b[0;36mh5py.h5d.create\u001b[0;34m()\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Unable to synchronously create dataset (name already exists)"],"ename":"ValueError","evalue":"Unable to synchronously create dataset (name already exists)","output_type":"error"}]}]}